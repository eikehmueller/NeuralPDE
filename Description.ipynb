{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "The goal is to construct a surrogate model that approximates the solution of a time-dependent PDE for a number of fields on the sphere. In abstract form this PDE is written as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\psi (x,t)}{\\partial t}  = \\mathcal{F}(\\psi(x,t))\n",
    "$$\n",
    "\n",
    "Here $\\mathcal{F}$ is a given (not necessarily linear) differential operator. For example, the PDE might describe the [time-dependent shallow water equations](https://en.wikipedia.org/wiki/Shallow_water_equations).\n",
    "\n",
    "Given the field $\\psi_0(x) = \\psi(x,t=0)$ we want to predict the field $\\psi(x,t=T)$ at the final time $T$.\n",
    "\n",
    "![Time evolution of the field](figures/time_evolution.svg)\n",
    "\n",
    "*Figure 1: time evolution of the field $\\psi(x,t)$ on the high-resolution mesh*\n",
    "\n",
    "### Input\n",
    "The input of the model is a tensor $X_{b,i,j}$ of shape $(B,n_{\\text{in}},n)$ where $B$ is the batch size. This tensor $X_{b,i,j}$ represents $n_{\\text{in}}$ functions, each of which is defined by $n$ dofs: for fixed sample index $b$ and field index $i$ the vector $X_{b,i,*}$ is the dof-vector of a Firedrake function. For simplicity we assume that all functions live in the same function space $V$ (if this wasn't the case $X_{b,i,j}$ might be a ragged tensor).\n",
    "\n",
    "The $n_{\\text{in}}$ input functions are further split into $n_{\\text{in}}^{\\text{(dyn)}}$ functions that represent *dynamic* quantities such as velocity, pressure etc. and $n_{\\text{in}}^{\\text{(ancil)}}$ functions that represent static *ancillary* quantities such as coordinates or orography. We thus write $X = (X^{\\text{(dyn)}},X^{\\text{(ancil)}})$ and obviously $n_{\\text{in}}=n_{\\text{in}}^{\\text{(dyn)}}+n_{\\text{in}}^{\\text{(ancil)}}$.\n",
    "\n",
    "### Output\n",
    "Similarly, the output of the model is a tensor $Y_{b,i,j}$ of shape $(B,n_{\\text{out}},n)$ which represents $n_{\\text{out}}$ functions, each of which is represented by $n$ dofs: for fixed sample index $b$ and field index $i$ the vector $Y_{b,i,*}$ is the dof-vector of a Firedrake function.\n",
    "\n",
    "The dof-vectors $Y^{(\\text{true})}_{b,i,j}$ of the true (or target) functions are obtained by solving a high-resolution model which integrates the discretised version of the PDE above and then possibly projects onto $n_{\\text{out}}\\le n_{\\text{in}}$ functions which represent the \"observed\" solution. If $X_{b,i,j}$ encodes the initial condition $\\psi(x,t=0)$ then $Y^{(\\text{true})}_{b,i,j}$ is obtained from the solution $\\psi(x,t=T)$ at some later time $T$ (see Fig. 1). Of course, $Y^{(\\text{true})}$ could also represent the \"true\" state of the dynamical system in the real world. This true state would be reconstructed from observations via data assimilation.\n",
    "\n",
    "### Model\n",
    "The goal is to find a learnable model $Y=\\Phi_\\theta(X)$ such that some loss $L(Y,Y^{(\\text{true})})$ is minimised. The simplest loss function is the MSE loss $L(Y,Y^{(\\text{true})}) = \\frac{1}{2}||Y-Y^{(\\text{true})}||^2$ which is proportional to the $L_2$ error in the function space $V$.\n",
    "\n",
    "In the following we describe this model in much more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure\n",
    "\n",
    "As in [Ryan Keisler's paper](https://arxiv.org/abs/2202.07575) and in [GraphCast](https://www.science.org/doi/epdf/10.1126/science.adi2336) the model $\\Phi_\\theta = \\mathcal{D}_{\\theta_D}\\circ\\mathcal{P}_{\\theta_P}\\circ\\mathcal{E}_{\\theta_E}$ is split into three components:\n",
    "\n",
    "1. the **encoder** $\\mathcal{E}_{\\theta_E}$ which maps the input tensor $X_{b,i,j}$ to the latent space via learnable embeddings\n",
    "2. a **processor** or **latent model** $\\mathcal{P}_{\\theta_P}$ which solves a system of coupled time-dependent ODEs in the latent space. The coupling structure of the system of ODEs is defined by the graph of a dual mesh and the forcing is a learned function that describes local interactions on the graph.\n",
    "3. the **decoder** $\\mathcal{D}_{\\theta_D}$ which maps the solution from the latent space back to the output tensor $Y_{b,i,j}$ via learnable embeddings\n",
    "\n",
    "The following figure summarises the structure of the model; the individual components are described in detail below.\n",
    "\n",
    "![Model structure](figures/model_structure.svg)\n",
    "*Figure 2: model structure*\n",
    "\n",
    "The main difference to [Ryan Keisler's paper](https://arxiv.org/abs/2202.07575) and [GraphCast](https://www.science.org/doi/epdf/10.1126/science.adi2336) is that the processor solves a time-dependent ODE instead of using message passing on a Graph Neural network. Hence, the model is a realisation of a [Neural ODE](https://arxiv.org/abs/1806.07366). However, the latent model only includes *local* interactions defined by the topology of the dual mesh. It can therefore be seen as a low-resolution discretisation of a learnable time-dependent PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent space\n",
    "The latent space is constructed as follows: consider the dual mesh of a refined icosahedron. This dual mesh has $n_{\\text{patch}}$ vertices and each vertex $\\alpha\\in0,1,\\dots,n_{\\text{patch}}-1$ has exactly three neighbours $\\beta\\in\\mathcal{N}(\\alpha)$. A state in the latent space is a tensor $Z_{b,\\alpha,k}$ of shape $(B,n_{\\text{patch}},d_{\\text{lat}})$ where we call $d_{\\text{lat}}$ is the dimension of the latent space. As for the input tensor, the tensor $Z$ is split into $d_{\\text{lat}}^{\\text{(dyn)}}$ dynamic components and $d_{\\text{lat}}^{\\text{(ancil)}}$ ancillary components in the latent dimension, so $Z=(Z^{\\text{(dyn)}},Z^{\\text{(ancil)}})$ and $d_{\\text{lat}}=d_{\\text{lat}}^{\\text{(dyn)}}+d_{\\text{lat}}^{\\text{(ancil)}}$.\n",
    "\n",
    "![dual mesh](figures/dual_mesh.svg)\n",
    "\n",
    "*Figure 3: Vertices and edges of the dual mesh*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Component I: Encoder\n",
    "The encoder consists of two parts:\n",
    "#### Projection to VOM\n",
    "For each $b,i$ the functions represented by $X_{b,i,*}$ are first projected to a vertex-only mesh (VOM). For each of the vertices of the dual mesh that defines the latent space we construct a little circular patch that consists of $p$ points, as shown in the following figure (in reality, however, the radii of the patches are adjusted such that they overlap and thus form a complete covering of the sphere). The patch covering is implemented in [spherical_patch_covering.py](src/neural_pde/spherical_patch_covering.py) where the dual mesh is obtained ny extracting the DMPlex data structure from the Firedrake mesh of a refined icosahedron.\n",
    "\n",
    "![patch covering of the sphere](figures/patch_covering.svg)\n",
    "\n",
    "*Figure 4: patch covering of the sphere with patches shown in red and dual mesh shown in black*\n",
    "\n",
    "The vertices of the VOM are then gives by the $n_{\\text{patch}}\\cdot p$ points of all vertices. We use Firedrake's interpolation functionality to implement the mapping $X_{b,i,j}\\mapsto \\overline{X}_{b,\\alpha,i,\\ell}$ where $\\overline{X}_{b,\\alpha,i,\\ell}$ is a tensor of shape $(B,n_{\\text{patch}},n_{\\text{in}},p)$. For given $b,i$ the projection to the VOM is a linear operator $\\mathcal{P}:\\mathbb{R}^n \\rightarrow \\mathbb{R}^{n_{\\text{patch}}\\times p}$\n",
    "\n",
    "$$\n",
    "\\overline{X}_{b,*,i,*} = \\mathcal{P} X_{b,i,*}\n",
    "$$\n",
    "\n",
    "Derivatives transform with respect to the adjoint $\\mathcal{P}^\\dagger$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X_{b,i,*}} =  \\mathcal{P}^\\dagger\\frac{\\partial}{\\partial \\overline{X}_{b,*,i,*}}\n",
    "$$\n",
    "\n",
    "and this operation is also available in Firedrake. The class `FunctionToPatchInterpolationLayer` in [patch_interpolation.py](src/neural_pde/patch_interpolation.py) uses this observation to implement the projection as a subclass of a tensorflow [keras layer](https://www.tensorflow.org/api_docs/python/tf/keras/.layers) through which we can back-propagate. The tensor $\\overline{X}=(\\overline{X}^{\\text{dyn}},\\overline{X}^{\\text{ancil}})$ is split into a *dynamic* and an *ancillary* component in the same way as the input tensor.\n",
    "\n",
    "#### Learnable embedding\n",
    "Next, the tensor $\\overline{X}_{b,\\alpha,i,\\ell}$ is converted to a state in latent space via a learnable embedding. This embedding is the same for all patches. For this, we treat the dynamic and the ancillary components differently. Let $E^{\\text{(dyn)}}_{\\theta_E}:\\mathbb{R}^{n_{\\text{in}}\\times P} \\rightarrow \\mathbb{R}^{d_{\\text{dyn}}}$ and $E^{\\text{(ancil)}}_{\\theta_E}:\\mathbb{R}^{n_{\\text{in}}^{\\text{(ancil)}}\\times P} \\rightarrow \\mathbb{R}^{d_{\\text{ancil}}}$ be the possibly non-linear encoder functions. Then for each sample $b$ and each patch $\\alpha$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z_{b,\\alpha,*}^{\\text{(dyn)}} &= E^{\\text{(dyn)}}_{\\theta_E} \\left( \\overline{X}^{\\text{(dyn)}}_{b,\\alpha,*,*},\\overline{X}^{\\text{(ancil)}}_{b,\\alpha,*,*}\\right)\\\\\n",
    "Z_{b,\\alpha,*}^{\\text{(ancil)}} &= E^{\\text{(ancil)}}_{\\theta_E} \\left( \\overline{X}^{\\text{(ancil)}}_{b,\\alpha,*,*}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the ancillary embedding depends only on the ancillary fields whereas the dynamic embedding depends on both the dynamic and the ancillary fields. The learnable embedding for the encoder is implemented in [patch_encoder.py](src/neural_pde/patch_encoder.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Component II: Processor\n",
    "In latent space we use the tensor $Z_{b,\\alpha,k}=(Z_{b,\\alpha,k}^{\\text{(dyn)}},Z_{b,\\alpha,k}^{\\text{(ancil)}})$ of shape $(B,n_{\\text{patch}},d_{\\text{lat}})$ as the initial condition for the following time-dependent ODE\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial Z^{\\text{(dyn)}}(t)}{\\partial t} &= \\mathcal{F}_{\\theta_P} (Z^{\\text{(dyn)}}(t),Z^{\\text{(ancil)}})\\qquad \\text{with $Z^{\\text{(dyn)}}(0) = Z^{\\text{(dyn)}}$}\\\\\n",
    "\\frac{\\partial Z^{\\text{(ancil)}}(t)}{\\partial t} &= 0\\qquad \\text{with $Z^{\\text{(ancil)}}(0) = Z^{\\text{(ancil)}}$}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}_{\\theta_P}$ is a learnable function. The structure of this function is dictated by the topology of the dual mesh that defines the latent space. More specifically, let $F_{\\theta_P}:\\mathbb{R}^{4,d_{\\text{lat}}}\\rightarrow \\mathbb{R}^{d_{\\text{lat}}^{\\text{(dyn)}}}$ be the function which describes the local interaction of the latent state vector at a vertex and its three direct neighbours. Then for each batch $b$ and vertex $\\alpha$ we have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Z_{b,\\alpha,*}(t)}{\\partial t} = F_{\\theta_P} \\left( (Z_{b,\\alpha,*}(t),Z_{b,\\beta_0,*}(t),Z_{b,\\beta_1,*}(t),Z_{b,\\beta_2,*}(t) )^\\top \\right)\n",
    "$$\n",
    "\n",
    "where $\\beta_0,\\beta_1,\\beta_2\\in\\mathcal{N}(\\alpha)$ are the indices of the three vertices that are direct neighbours of the vertex $\\alpha$ in the dual mesh, as shown in the following figure.\n",
    "\n",
    "![local interaction](figures/interaction.svg)\n",
    "\n",
    "*Figure 5: local interaction with the function $F_{\\theta_P}$ at vertex $\\alpha$. In this example the 5-dimensional state vectors at the vertices are represented by the little boxes for a latent space with $d_{\\text{lat}}^{\\text{(dyn)}}=3$ (indicated in red) and $d_{\\text{lat}}^{\\text{(ancil)}}=2$ (indicated in blue)*\n",
    "\n",
    "In practice the time-dependent ODE is solved with a numerical timestepping method; the current implementation uses a simple forward-Euler scheme.\n",
    "\n",
    "The output of the processor is the solution $Z'_{b,\\alpha,k}=Z_{b,\\alpha,k}(\\tau)$ at the final latent time $\\tau$. The processor is implemented in [neural_solver.py](src/neural_pde/neural_solver.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Component III: Decoder\n",
    "Like the encoder, the decoder is also split into two parts.\n",
    "\n",
    "#### Learnable embedding\n",
    "First the tensor $Z'_{b,\\alpha,k}$ of shape $(B,n_{\\text{patch}},d_{\\text{lat}})$ is mapped to a tensor $\\overline{Y}_{b,\\alpha,i,\\ell}$ of shape $(B,n_{\\text{patch}},n_{\\text{out}},p)$ on the VOM via a learnable embedding. For this let $D_{\\theta_D}:\\mathbb{R}^{d_{\\text{lat}}} \\rightarrow \\mathbb{R}^{n_{\\text{out}}\\times P}$ be a learnable function. Then for each sample $b$ and each patch $\\alpha$\n",
    "\n",
    "$$\n",
    "\\overline{Y}_{b,\\alpha,*,*} = D_{\\theta_D}\\left(Z'_{b,\\alpha,*}\\right)\n",
    "$$\n",
    "\n",
    "The learnable embedding for the decoder is implemented in [patch_decoder.py](src/neural_pde/patch_decoder.py).\n",
    "\n",
    "#### Reconstruction on original mesh\n",
    "To recover the output tensor $Y_{b,i,j}$ of shape $(B,n_{\\text{out}},n)$ which represents the solution functions on the original mesh we use the adjoint of the map $\\mathcal{P}$ that is used for the projection to the VOM in the encoder. For each sample $b$ and each output field $i$\n",
    "\n",
    "$$\n",
    "Y_{b,i,*} = \\mathcal{P}^\\dagger \\overline{Y}_{b,*,i,*}\n",
    "$$\n",
    "\n",
    "Now derivatives transform with $\\mathcal{P}$ itself:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial} {\\partial \\overline{Y}_{b,*,i,*}} = \\mathcal{P}\\frac{\\partial}{\\partial Y_{b,i,*}}.\n",
    "$$\n",
    "\n",
    "The class `PatchToFunctionInterpolationLayer` in [patch_interpolation.py](src/neural_pde/patch_interpolation.py) implements this reconstruction as a subclass of a tensorflow [keras layer](https://www.tensorflow.org/api_docs/python/tf/keras/.layers) through which we can back-propagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary of learnable functions\n",
    "The learnable functions in the following table are represened by small neural networks, i.e. instances of a tensorflow [keras model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n",
    "\n",
    "| component | function | from | to |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| encoder (dynamic) | $E^{\\text{(dyn)}}_{\\theta_E}$ | $\\mathbb{R}^{n_{\\text{in}}\\times P}$ | $\\mathbb{R}^{d_{\\text{dyn}}}$ |\n",
    "| encoder (ancillary) | $E^{\\text{(ancil)}}_{\\theta_E}$ | $\\mathbb{R}^{n_{\\text{in}}^{\\text{(ancil)}}\\times P}$ | $\\mathbb{R}^{d_{\\text{ancil}}}$ |\n",
    "| processor forcing function | $F_{\\theta_P}$ | $\\mathbb{R}^{4,d_{\\text{lat}}}$ | $ \\mathbb{R}^{d_{\\text{lat}}^{\\text{(dyn)}}}$ |\n",
    "| decoder | $D_{\\theta_D}$ | $\\mathbb{R}^{d_{\\text{lat}}}$ | $\\mathbb{R}^{n_{\\text{out}}\\times P}$ |\n",
    "\n",
    "*Table 1: list of learnable functions*\n",
    "\n",
    "## Putting it all together\n",
    "Since all components of the model are subclasses of tensorflow [keras.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/.layers), the full model can be constructed for example as a [keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model. See [train.py](src/train.py) for an example of a full traning pipeline. This utilises the data generator classes in [data_generator.py](src/neural_pde/data_generator.py), which at the moment just implement the very simple PDE model of a solid body rotation. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
